export default {
  common: {
    save: "Save",
    cancel: "Cancel",
    delete: "Delete",
    edit: "Edit",
    create: "Create",
    add: "Add",
    enable: "Enable",
    disable: "Disable",
    enabled: "Enabled",
    disabled: "Disabled",
    loading: "Loading...",
    noData: "No data",
    success: "Success",
    error: "Error",
    confirm: "Confirm",
    copy: "Copy",
    test: "Test",
    import: "Import",
    export: "Export",
    search: "Search",
    name: "Name",
    type: "Type",
    status: "Status",
    actions: "Actions",
    back: "Back",
    close: "Close",
    active: "Active",
    inactive: "Inactive",
  },
  nav: {
    providers: "Providers",
    mcpServers: "MCP Servers",
    skills: "Skills",
    prompts: "Prompts",
    export: "Export",
    cliGuide: "CLI Guide",
    skillsGuide: "Skills Guide",
    setupGuide: "Setup Guide",
    aiGlossary: "AI Tech",
    management: "Management",
    platform: "Config Platform",
  },
  auth: {
    login: "Login",
    register: "Register",
    email: "Email",
    password: "Password",
    displayName: "Display Name",
    loginTitle: "Sign in to your account",
    registerTitle: "Create new account",
    switchToRegister: "No account? Register",
    switchToLogin: "Have an account? Login",
    registerSuccess: "Registration successful, please check your email",
    loginSuccess: "Login successful",
    logout: "Logout",
  },
  providers: {
    title: "Provider Management",
    subtitle: "Manage AI CLI API configurations",
    addProvider: "Add Provider",
    editProvider: "Edit Provider",
    empty: "No Providers",
    emptyHint: "Click \"Add Provider\" to get started",
    providerName: "Provider Name",
    providerType: "Type",
    appType: "Application",
    apiKey: "API Key",
    baseUrl: "Base URL",
    official: "Official Login",
    custom: "Custom",
    createSuccess: "Created successfully",
    updateSuccess: "Updated successfully",
    deleteSuccess: "Deleted successfully",
    copySuccess: "Copied successfully",
    testSuccess: "Connection successful",
    testFailed: "Connection failed",
    notSet: "Not set",
  },
  mcp: {
    title: "MCP Server Management",
    subtitle: "Manage MCP server configurations",
    addServer: "Add MCP Server",
    editServer: "Edit MCP Server",
    empty: "No MCP Servers",
    emptyHint: "Click \"Add MCP Server\" to get started",
    serverName: "MCP Server Name",
    transportType: "Transport Type",
    command: "Command",
    arguments: "Arguments (space separated)",
    url: "URL",
    envVars: "Environment Variables",
    addEnvVar: "Add Variable",
    appBindings: "App Bindings",
    createSuccess: "Created successfully",
    updateSuccess: "Updated successfully",
    deleteSuccess: "Deleted successfully",
    testPassed: "Test passed",
    testFailed: "Test failed",
  },
  skills: {
    title: "Skills Management",
    subtitle: "Manage AI CLI skill packs",
    addRepo: "Add Repository",
    editRepo: "Edit Repository",
    repoOwner: "Owner",
    repoName: "Repository",
    branch: "Branch",
    subdirectory: "Subdirectory (optional)",
    isDefault: "Default Repository",
    scanSkills: "Scan Skills",
    scanning: "Scanning...",
    installed: "Installed",
    notInstalled: "Not Installed",
    install: "Install",
    uninstall: "Uninstall",
    emptyRepos: "No repositories",
    emptyReposHint: "Add GitHub repositories to discover Skills",
    emptySkills: "No Skills",
    emptySkillsHint: "Click \"Scan Skills\" to discover skill packs",
    scanSuccess: "Scan complete, found {count} skills",
    scanFailed: "Scan failed",
  },
  prompts: {
    title: "Prompts Management",
    subtitle: "Manage system prompt presets & AI optimization",
    addPrompt: "Add Prompt",
    editPrompt: "Edit Prompt",
    empty: "No Prompts",
    emptyHint: "Click \"Add Prompt\" to create a preset",
    promptName: "Prompt Name",
    targetFile: "Target File",
    content: "Content (Markdown)",
    editTab: "Edit",
    previewTab: "Preview",
    setActive: "Set Active",
    activated: "Active",
    noContent: "(No content)",
    createSuccess: "Created successfully",
    updateSuccess: "Updated successfully",
    deleteSuccess: "Deleted successfully",
    managementTab: "Prompts Management",
    optimizerTab: "Prompt Optimizer",
  },
  export: {
    title: "Export Config",
    subtitle: "Export configurations in each CLI's official format",
    appExport: "App Export (ZIP)",
    appExportDesc: "Generate config files strictly following each CLI's official format, packaged as a ZIP. Extract and place files at the indicated paths.",
    appExportHint: "After extracting the ZIP, place files at the specified paths. Claude Code's Provider env fields (API Key, Base URL, Model) are merged into settings.json. Codex MCP servers are embedded directly in config.toml as [mcp_servers.name] tables.",
    dataBackup: "Data Backup",
    dataBackupDesc: "Backup app data for cross-account migration or restore ‚Äî not for direct use as CLI config files",
    fullBackup: "Full Backup (ZIP)",
    backupDesc: "This backup format is only for restoring data within this app, not a native CLI format",
    backupWarning: "Backup files are for in-app import/restore only. Do not place them directly in CLI config directories.",
    import: "Import Config",
    importDesc: "Restore data from JSON files. Supports Providers / MCP Servers / Prompts formats",
    importHint: "Supports importing Providers, MCP Servers, and Prompts JSON format files",
    deepLink: "Deep Link",
    deepLinkDesc: "Generate share links for quick Provider import",
    exporting: "Exporting...",
    exportSuccess: "Export successful",
    importSuccess: "Import successful, {count} records imported",
    importFailed: "Import failed",
    selectFile: "Select File",
    generateLink: "Generate Link",
    copyLink: "Copy Link",
    linkCopied: "Link copied",
    providers: "Providers",
    mcpServers: "MCP Servers",
    skills: "Skills",
    prompts: "Prompts",
  },
  help: {
    title: "User Guide",
  },
  helpProviders: {
    what: "What is a Provider?",
    whatDesc: "A Provider is an API configuration for AI CLI tools to connect to AI services. The platform supports unified management of Providers for Claude Code, Codex, Gemini CLI, OpenCode, etc. Each Provider contains API Key, Base URL, application type, and more.",
    whatTip: "You can configure multiple Providers per app. The first enabled Provider (by sort order) is used during export.",
    types: "Provider Types",
    typesDesc: "‚Ä¢ Official Login: Use official account (Anthropic/OpenAI), no API Key needed. Only for apps supporting OAuth\n‚Ä¢ PackyCode: Third-party API proxy. Requires API Key from PackyCode. Base URL auto-filled\n‚Ä¢ Custom: Fully custom API endpoint for self-hosted proxies or other API providers",
    typesTip: "When using Official Login or PackyCode, the Base URL is auto-filled based on the selected app type.",
    howTo: "How to Configure?",
    howToDesc: "1. Click \"Add Provider\" and choose a preset template or fill in manually\n2. Select the target application (Claude / Codex / Gemini / OpenCode)\n3. Enter API Key (can be left empty for Official Login)\n4. Custom type requires manual Base URL entry\n5. Toggle controls whether this Provider is included in exports\n6. Use the copy button to quickly clone existing configurations",
    test: "Connection Test",
    testDesc: "Click the test icon (lightning/signal) on the card. The system sends a probe request to verify network connectivity and API Key validity.\n\n‚Ä¢ Green ‚úì ‚Äî Connection successful, latency shown\n‚Ä¢ Red ‚úó ‚Äî Connection failed, with specific reason (timeout, 401 unauthorized, network unreachable, etc.)",
    testTip: "Testing only checks endpoint reachability and Key validity ‚Äî no API credits are consumed. Official Login type cannot be tested remotely.",
  },
  helpMcp: {
    what: "What is an MCP Server?",
    whatDesc: "MCP (Model Context Protocol) Servers provide AI tools with extended capabilities like file I/O, web requests, database queries, browser automation, and more. By configuring MCP Servers, AI tools can invoke external services for complex tasks.",
    whatTip: "MCP is an open protocol by Anthropic, adopted by Claude Code, Codex, Gemini CLI, and other tools.",
    transport: "Transport Types",
    transportDesc: "‚Ä¢ Stdio (Standard I/O): Starts a local process via command, communicates through stdin/stdout. Best for most scenarios like npx-based MCP services\n‚Ä¢ HTTP: Connects to a remote HTTP endpoint. For cloud-deployed MCP services\n‚Ä¢ SSE (Server-Sent Events): Communicates via event streams. For real-time push scenarios",
    transportTip: "Most MCP Servers use Stdio. Choose HTTP or SSE for remotely deployed services.",
    templates: "Template Library",
    templatesDesc: "The platform provides one-click templates in 5 categories:\n\nüåê Browser & Testing: Playwright, Puppeteer, etc.\nüîç Search & Network: mcp-fetch, Brave Search, Context7, etc.\nüíæ Data & Storage: SQLite, PostgreSQL, filesystem, memory, etc.\nüõ†Ô∏è Dev Tools: GitHub, Sequential Thinking, etc.\nüí¨ Collaboration: Slack, etc.\n\nClick a template name to instantly create the MCP Server configuration.",
    templatesTip: "Templates are fully editable after creation. Some (like databases) require path parameter modifications.",
    env: "Environment Variables",
    envDesc: "Configure runtime environment variables for MCP Servers, such as:\n‚Ä¢ API Token / Secret Key ‚Äî for authentication\n‚Ä¢ File paths ‚Äî specify working directories\n‚Ä¢ Database connection strings ‚Äî specify data sources\n\nVariables are set as key-value pairs. Multiple variables supported.",
    envTip: "Use environment variables for sensitive info (API Keys) rather than hardcoding in command arguments.",
    bindings: "App Bindings",
    bindingsDesc: "Each MCP Server can be bound to one or more target apps (Claude / Codex / Gemini / OpenCode).\n\n‚Ä¢ Bound apps will include this MCP Server during app-specific export\n‚Ä¢ Unbound apps won't include it\n‚Ä¢ Full export always includes all MCP Servers regardless of bindings",
    bindingsTip: "Check all relevant apps if an MCP Server should be available across multiple tools.",
  },
  helpSkills: {
    what: "What are Skills?",
    whatDesc: "Skills are predefined AI skill packs, typically hosted in GitHub repositories. Each Skill contains domain-specific instructions, context, and best practices to help AI tools better handle specific tasks (code review, test generation, architecture design, etc.).",
    whatTip: "Skills were originally designed by Anthropic for Claude Code. This platform extends support for Skills from any GitHub repository.",
    repos: "Repository Management",
    reposDesc: "Add GitHub repositories as Skills sources. Each repo config includes:\n‚Ä¢ Owner + Repo name ‚Äî e.g., anthropics/skills\n‚Ä¢ Branch ‚Äî defaults to main, can specify others\n‚Ä¢ Subdirectory ‚Äî if Skills aren't in the repo root\n‚Ä¢ Default flag ‚Äî marked repos are displayed first\n\nBuilt-in preset categories (Skills repos, Dev, Design, Office, etc.) allow quick addition.",
    reposTip: "Private repos may require GitHub API authorization. Public repos need no authentication.",
    scan: "Scan & Install",
    scanDesc: "1. After adding a repo, click the \"Scan Skills\" button on the repo card\n2. The system reads the repo's directory structure via GitHub API, discovering available Skills (each subdirectory = one Skill)\n3. It also tries to read each Skill's README.md for descriptions\n4. Discovered Skills appear in the \"Skills\" tab\n5. Use toggles to install/uninstall ‚Äî installed Skills are included in exports",
    scanTip: "Scanning uses GitHub API with a 60 requests/hour limit (unauthenticated). For large repos, specify a subdirectory to narrow the scan.",
    views: "Views & Filtering",
    viewsDesc: "The Skills list supports two view modes:\n‚Ä¢ Card view ‚Äî grid layout for visual overview\n‚Ä¢ List view ‚Äî table layout for dense information\n\nFilter by name/description search, installation status, or source repository.",
  },
  helpPrompts: {
    what: "What are Prompts?",
    whatDesc: "Prompts are system prompt presets that guide AI CLI tool behavior and style. Different AI tools use different config files for system prompts. This platform provides unified management and app-specific export.",
    whatTip: "System prompts define AI behavior preferences, coding style, response language, etc. Proper configuration significantly improves the experience.",
    target: "Target File Mapping",
    targetDesc: "Each Prompt selects a target file that determines the exported filename:\n\n‚Ä¢ CLAUDE.md ‚Üí Claude Code ‚Äî placed in project root, auto-loaded on startup\n‚Ä¢ AGENTS.md ‚Üí Codex ‚Äî OpenAI Codex CLI system prompt file\n‚Ä¢ GEMINI.md ‚Üí Gemini CLI ‚Äî Google Gemini CLI custom instructions\n‚Ä¢ OPENCODE.md ‚Üí OpenCode ‚Äî OpenCode CLI prompt configuration",
    targetTip: "CLAUDE.md supports hierarchical loading: root-level applies globally, subdirectory-level applies only within that directory.",
    editor: "Editor Features",
    editorDesc: "Prompt content uses Markdown format, supporting:\n‚Ä¢ Heading levels (# ## ###)\n‚Ä¢ Lists (ordered/unordered)\n‚Ä¢ Code blocks (inline and block)\n‚Ä¢ Emphasis (bold/italic)\n‚Ä¢ Links and quotes\n\nSwitch to the \"Preview\" tab for live Markdown rendering. Content limit: 50,000 characters.",
    editorTip: "Use clear heading structure to organize prompts: # Role Definition ‚Üí # Coding Standards ‚Üí # Output Format.",
    active: "Activation Management",
    activeDesc: "Each Prompt can be individually activated/deactivated:\n\n‚Ä¢ Active Prompts are automatically used as target file content during app-specific export\n‚Ä¢ Multiple Prompts can exist per target file, but only one should be active to avoid conflicts\n‚Ä¢ Deactivated Prompts are preserved and can be reactivated anytime\n‚Ä¢ The card toggle allows quick activation switching",
    activeTip: "Create multiple Prompts per tool (e.g., \"Daily Dev\", \"Code Review\", \"Doc Writing\") and switch activation by scenario.",
    optimizer: "Prompt Optimizer",
    optimizerDesc: "Built-in AI-powered prompt optimization to help you:\n‚Ä¢ Analyze existing prompt structure and coverage\n‚Ä¢ Auto-optimize wording and fill missing key instructions\n‚Ä¢ Reorganize content following best practice templates\n‚Ä¢ Support feedback and iteration on optimization results",
    optimizerTip: "The optimizer uses industry best practices, but final effectiveness depends on your use case. Review carefully before applying.",
  },
  helpExport: {
    what: "Export Overview",
    whatDesc: "Export platform configurations as official-format files ready for each AI CLI tool. App Export generates standard ZIP packages ‚Äî extract and place files at the specified paths.",
    whatTip: "App Export (ZIP) is the recommended method. Files strictly follow official CLI specs and are auto-recognized on CLI startup.",
    app: "App Export (ZIP)",
    appDesc: "Each CLI tool generates a ZIP with all required config files:\n\n‚Ä¢ Claude Code ‚Üí settings.json ($schema + env{API Key/URL/Model} + permissions + mcpServers) + CLAUDE.md + skills/\n  Path: place settings.json at %USERPROFILE%\\.claude\\settings.json (Windows) or ~/.claude/settings.json (macOS/Linux)\n\n‚Ä¢ Codex CLI ‚Üí config.toml (TOML format: top-level model/api_key + [mcp_servers.name] inline tables) + AGENTS.md\n  Path: place config.toml at %USERPROFILE%\\.codex\\config.toml (Windows) or ~/.codex/config.toml (macOS/Linux)\n\n‚Ä¢ Gemini CLI ‚Üí settings.json (mcpServers) + GEMINI.md\n  Path: place settings.json at ~/.gemini/settings.json\n\n‚Ä¢ OpenCode ‚Üí config.json + OPENCODE.md\n\nOnly exports: enabled Providers, app-bound MCP Servers, active Prompts, installed Skills.",
    appTip: "Before exporting, ensure: Provider is enabled, MCP Servers are bound, Prompt is activated, Skills are installed.\n\nImportant: Claude Code's API Key (ANTHROPIC_AUTH_TOKEN), Base URL (ANTHROPIC_BASE_URL), and model name are written directly into settings.json's env field ‚Äî not a separate shell script. Codex MCP Servers are embedded in config.toml as [mcp_servers.name] tables ‚Äî no separate mcp.json or config.yaml is generated.",
    backup: "Data Backup",
    backupDesc: "Export all module data as JSON (internal fields like id/user_id/timestamps stripped), useful for:\n‚Ä¢ Cross-account data migration\n‚Ä¢ Data backup and restore\n‚Ä¢ Bulk import within this app\n\n‚ö†Ô∏è Backup format is NOT a native CLI config format ‚Äî do not place in CLI config directories.",
    backupTip: "Use the Import function to restore backup data within this app. Great for switching accounts or multi-device sync.",
    import: "Import Config",
    importDesc: "Restore data from JSON files, auto-detected by type:\n‚Ä¢ Providers ‚Äî contains provider_type field\n‚Ä¢ MCP Servers ‚Äî contains transport_type field\n‚Ä¢ Prompts ‚Äî contains target_file field\n\nImport appends records without overwriting existing data.",
    importTip: "Import files must be JSON arrays. Use the Data Backup feature to generate standard format files.",
    deepLink: "Deep Link Sharing",
    deepLinkDesc: "Generate a shareable link containing enabled Provider configs (excluding API Keys). Recipients can import with one click.",
    deepLinkTip: "Deep Links exclude sensitive info like API Keys. Recipients need to enter their own keys.",
  },
  cliGuide: {
    title: "CLI Command Reference",
    subtitle: "Complete reference for Claude Code, Codex CLI, and Gemini CLI built-in commands",
    searchPlaceholder: "Search commands...",
    expandAll: "Expand All",
    collapseAll: "Collapse All",
    officialDocs: "Official Docs",
    showing: "Showing",
    total: "Total",
    commands: "commands",
    noResults: "No matching commands found",
  },
  skillsGuide: {
    title: "Skills Usage Guide",
    subtitle: "Detailed Agent Skills documentation and configuration manual for Claude Code, Codex CLI, and Gemini CLI",
    searchPlaceholder: "Search skill docs...",
    expandAll: "Expand All",
    collapseAll: "Collapse All",
    officialDocs: "Official Docs",
    showing: "Showing",
    total: "Total",
    items: "items",
    noResults: "No matching items found",
  },
  setupGuide: {
    title: "Environment Setup & Tips",
    subtitle: "Setup guides and best practices for Anthropic / Codex / Gemini CLI",
    searchPlaceholder: "Search setup docs...",
    expandAll: "Expand All",
    collapseAll: "Collapse All",
    officialDocs: "Official Docs",
    showing: "Showing",
    total: "Total",
    items: "items",
    noResults: "No matching items found",
  },
  aiGlossary: {
    title: "AI Frontier Technology Overview",
    subtitle: "Comprehensive guide to core LLM concepts, cutting-edge technologies, and mainstream model classifications",
    searchPlaceholder: "Search concepts...",
    expandAll: "Expand All",
    collapseAll: "Collapse All",
    showing: "Showing",
    total: "Total",
    items: "concepts",
    noResults: "No matching concepts found",
    tabs: {
      agentSystem: "Agent System",
      protocols: "Protocols & Services",
      methods: "Technical Methods",
      models: "AI Models",
    },
    agent: {
      title: "Agent",
      subtitle: "Autonomous AI decision-making entity",
      definition: "Definition",
      definitionContent: "An Agent is an autonomous decision-making entity based on Large Language Models (LLMs), capable of perceiving its environment, reasoning, and taking actions. It receives user instructions or environmental signals, autonomously plans task steps, invokes tools, and iteratively optimizes based on feedback. The core of an Agent lies in its autonomy ‚Äî completing complex multi-step tasks without step-by-step human guidance.",
      function: "Functions",
      functionContent: "Agents solve the limitations of traditional LLM \"single Q&A\" patterns:\n‚Ä¢ Autonomously decompose complex tasks into sub-steps\n‚Ä¢ Dynamically invoke external tools (file systems, APIs, databases, etc.)\n‚Ä¢ Reflect and self-correct based on execution results\n‚Ä¢ Maintain task context across multiple interaction rounds",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ Code development assistants: Claude Code, Codex CLI, Cursor Agent ‚Äî autonomously read codebases, write code, run tests\n‚Ä¢ Customer service automation: understand queries, search knowledge bases, execute ticket operations\n‚Ä¢ Data analysis: write SQL queries, generate visualizations, compose analysis reports\n‚Ä¢ Research assistants: retrieve literature, summarize papers, generate research reviews",
      relation: "Relationships",
      relationContent: "Agent is the fundamental unit of the AI Agent ecosystem:\n‚Ä¢ Foundation for Sub-Agent and Agent Team\n‚Ä¢ Extends capabilities by loading Skills\n‚Ä¢ Invokes external tools via MCP protocol\n‚Ä¢ Communicates with other Agents via ACP protocol\n‚Ä¢ Behavior driven and defined by Prompt Engineering\n‚Ä¢ Orchestrated as execution nodes within Workflows",
    },
    subAgent: {
      title: "Sub-Agent",
      subtitle: "Derived agent focused on subtasks",
      definition: "Definition",
      definitionContent: "A Sub-Agent is a specialized agent dynamically created by a main Agent, dedicated to executing specific subtasks. The main Agent decomposes complex tasks and spawns a Sub-Agent for each subtask, each with independent context and tool access. Results are returned to the main Agent for aggregation ‚Äî similar to a \"fork\" operation in programming.",
      function: "Functions",
      functionContent: "Sub-Agents address capability bottlenecks when a single Agent handles complex tasks:\n‚Ä¢ Task decomposition and parallel processing for improved efficiency\n‚Ä¢ Reduced complexity: each Sub-Agent focuses on a single subtask\n‚Ä¢ Risk isolation: subtask failures don't affect other parallel tasks\n‚Ä¢ Specialized handling: different Sub-Agents can load different Skills and tools",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ Claude Code's context: fork ‚Äî main Agent spawns Sub-Agents for parallel multi-file editing\n‚Ä¢ Multi-file code refactoring: process different modules separately\n‚Ä¢ Parallel test execution: run multiple test suites simultaneously\n‚Ä¢ Batch document processing: parallel translation, proofreading, formatting",
      relation: "Relationships",
      relationContent: "Sub-Agent serves as the \"executor\" in the Agent hierarchy:\n‚Ä¢ Created, scheduled, and lifecycle-managed by the parent Agent\n‚Ä¢ Multiple Sub-Agents can form an Agent Team\n‚Ä¢ Orchestrated via Workflow for execution order and dependencies\n‚Ä¢ Can independently load Skills for specialized capabilities\n‚Ä¢ Results aggregated by the main Agent for final decisions",
    },
    agentTeam: {
      title: "Agent Team",
      subtitle: "Multi-Agent collaborative architecture",
      definition: "Definition",
      definitionContent: "An Agent Team is a collaborative architecture of multiple Agents, each assuming specific roles (architect, developer, tester, reviewer). They work together through predefined collaboration processes. Team members can be independent main Agents or Sub-Agents ‚Äî the key is role specialization and communication mechanisms.",
      function: "Functions",
      functionContent: "Agent Teams tackle large-scale complex tasks beyond single Agent capability:\n‚Ä¢ Role specialization: each Agent focuses on its area of expertise\n‚Ä¢ Multi-perspective review: different roles check quality from different angles\n‚Ä¢ Pipeline collaboration: one Agent's output feeds the next\n‚Ä¢ Scalability: add team members for larger-scale tasks",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ Software dev team: Architect Agent designs ‚Üí Developer Agent codes ‚Üí Tester Agent tests ‚Üí Reviewer Agent reviews\n‚Ä¢ Research team: Literature retrieval Agent ‚Üí Data analysis Agent ‚Üí Paper writing Agent\n‚Ä¢ Content team: Planning Agent ‚Üí Writing Agent ‚Üí Editing Agent ‚Üí Publishing Agent\n‚Ä¢ DevOps team: Monitoring Agent ‚Üí Diagnosis Agent ‚Üí Fix Agent ‚Üí Verification Agent",
      relation: "Relationships",
      relationContent: "Agent Team is the advanced collaboration pattern:\n‚Ä¢ Composed of multiple Agents and/or Sub-Agents\n‚Ä¢ Uses ACP protocol for inter-member communication and coordination\n‚Ä¢ Workflow defines collaboration processes and task assignment\n‚Ä¢ Each Agent can load different Skills for role differentiation\n‚Ä¢ Team accesses shared external resources via MCP Servers",
    },
    skills: {
      title: "Skills",
      subtitle: "Reusable Agent capability extension packs",
      definition: "Definition",
      definitionContent: "Skills are predefined, reusable capability packs that extend an Agent's domain expertise and behavior patterns. Each Skill contains domain-specific instructions, context, best practices, and constraints. Once loaded, the Agent gains corresponding professional capabilities. Skills enable modular and progressive disclosure of Agent abilities.",
      function: "Functions",
      functionContent: "Skills provide standardized extension mechanisms for Agent capabilities:\n‚Ä¢ Modular reuse: write once, use everywhere\n‚Ä¢ Progressive disclosure: load on demand to avoid context overload\n‚Ä¢ Domain specialization: package expert knowledge into distributable packs\n‚Ä¢ Version management: manage Skill iterations via Git repositories\n‚Ä¢ Community sharing: developers can share and reuse Skills",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ Claude Code Skills: install community Skills via /install-skill for code review, test generation, etc.\n‚Ä¢ Codex Skills: custom instruction sets guiding coding style and standards\n‚Ä¢ Gemini Skills: configure scenario-specific behavior preferences\n‚Ä¢ Enterprise Skills: encapsulate team coding standards, architecture patterns, security policies",
      relation: "Relationships",
      relationContent: "Skills form the extension layer of Agent capabilities:\n‚Ä¢ Loaded and invoked by Agents on demand\n‚Ä¢ Can leverage MCP Servers for lower-level tool capabilities\n‚Ä¢ Skill content is essentially well-crafted Prompt Engineering\n‚Ä¢ In Agent Teams, different Agents load different Skills for role differentiation\n‚Ä¢ Managed and distributed via Git repositories",
    },
    mcp: {
      title: "MCP Servers",
      subtitle: "Standardized Agent tool invocation interface",
      definition: "Definition",
      definitionContent: "MCP (Model Context Protocol) is an open protocol standard proposed by Anthropic. MCP Servers are the server-side implementation providing standardized external tool invocation interfaces for Agents. They enable unified access to file systems, databases, APIs, browsers, and more. MCP uses client-server architecture supporting Stdio, HTTP, and SSE transport methods.",
      function: "Functions",
      functionContent: "MCP Servers solve the standardization of Agent-external world interaction:\n‚Ä¢ Unified interface: different tools and services exposed through one protocol\n‚Ä¢ Plug-and-play: Agents don't need to understand tool internals\n‚Ä¢ Security isolation: MCP Servers run in independent processes\n‚Ä¢ Ecosystem expansion: anyone can develop and publish MCP Servers\n‚Ä¢ Cross-platform: supported by Claude Code, Codex, Gemini CLI, and more",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ File system operations: read/write, search, monitor file changes\n‚Ä¢ Database queries: connect PostgreSQL, SQLite, MongoDB, etc.\n‚Ä¢ Browser automation: Playwright, Puppeteer-driven web operations\n‚Ä¢ API integration: GitHub, Slack, JIRA, and other third-party services\n‚Ä¢ Search services: Brave Search, Google Search engine access\n‚Ä¢ Knowledge retrieval: connect RAG systems for knowledge base queries",
      relation: "Relationships",
      relationContent: "MCP Servers are the infrastructure of the Agent tool ecosystem:\n‚Ä¢ Invoked by Agents via MCP protocol for external tool capabilities\n‚Ä¢ Complementary to ACP ‚Äî MCP handles Agent-tool communication, ACP handles Agent-Agent communication\n‚Ä¢ Can serve as the underlying capability provider for Skills\n‚Ä¢ RAG systems can expose retrieval capabilities through MCP Servers\n‚Ä¢ Orchestrated as tool nodes within Workflows",
    },
    acp: {
      title: "ACP (Agent Communication Protocol)",
      subtitle: "Standardized inter-Agent communication",
      definition: "Definition",
      definitionContent: "ACP (Agent Communication Protocol) is a standardized communication protocol designed for multi-Agent systems. It defines specifications for Agent discovery, negotiation, message passing, and collaboration. ACP enables Agents built on different platforms by different developers to identify and collaborate with each other ‚Äî similar to how HTTP standardizes web communication.",
      function: "Functions",
      functionContent: "ACP solves interoperability in multi-Agent systems:\n‚Ä¢ Agent discovery: find other available Agents and their capabilities\n‚Ä¢ Capability negotiation: Agents negotiate task assignments\n‚Ä¢ Message passing: define message formats and semantics\n‚Ä¢ State synchronization: maintain consistency across Agents\n‚Ä¢ Cross-platform interop: Agents from different frameworks collaborate seamlessly",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ Cross-platform Agent interop: Claude Agent collaborating with GPT Agent\n‚Ä¢ Agent marketplace: publish and discover available Agent services\n‚Ä¢ Enterprise multi-Agent orchestration: coordinate Agents across departments\n‚Ä¢ Federated learning: Agents collaborating while preserving data privacy",
      relation: "Relationships",
      relationContent: "ACP is the communication foundation for multi-Agent collaboration:\n‚Ä¢ Provides inter-member communication for Agent Teams\n‚Ä¢ Complementary to MCP ‚Äî MCP for Agent-tool, ACP for Agent-Agent communication\n‚Ä¢ Supports cross-Agent task transfer and state sync in Workflows\n‚Ä¢ Enables Agents with different Skills to understand and collaborate",
    },
    lsp: {
      title: "LSP (Language Server Protocol)",
      subtitle: "AI-enhanced code comprehension",
      definition: "Definition",
      definitionContent: "LSP (Language Server Protocol) was originally designed by Microsoft for IDEs, providing code completion, go-to-definition, error diagnostics, and more. In AI Agent scenarios, LSP is extended as the Agent's \"code understanding engine,\" enabling Agents to comprehend code structure, symbol relationships, and type systems like human developers ‚Äî not just treating code as text.",
      function: "Functions",
      functionContent: "LSP provides structured code comprehension for AI Agents:\n‚Ä¢ Symbol resolution: understand variable, function, class definitions and references\n‚Ä¢ Type inference: obtain expression type information to reduce errors\n‚Ä¢ Go-to-definition: trace symbols to their definitions\n‚Ä¢ Error diagnostics: real-time syntax and type error detection\n‚Ä¢ Code completion: context-aware intelligent suggestions\n‚Ä¢ Refactoring support: safe renaming, method extraction, and more",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ AI coding assistant code analysis: Claude Code uses LSP for project structure understanding\n‚Ä¢ Smart refactoring: Agents perform safe cross-file refactoring based on LSP\n‚Ä¢ Code review: leverage LSP diagnostics to identify potential issues\n‚Ä¢ Project navigation: Agents quickly locate relevant code via LSP",
      relation: "Relationships",
      relationContent: "LSP enhances Agent expertise in the code domain:\n‚Ä¢ Provides structured code comprehension beyond plain text analysis\n‚Ä¢ Utilized by Skills ‚Äî code-related Skills depend on LSP analysis\n‚Ä¢ Can be exposed via MCP Servers for remote Agent access\n‚Ä¢ Improves Prompt Engineering effectiveness with LSP-informed prompts",
    },
    rag: {
      title: "RAG (Retrieval-Augmented Generation)",
      subtitle: "Knowledge-enhanced LLM output",
      definition: "Definition",
      definitionContent: "RAG (Retrieval-Augmented Generation) is a technical architecture combining external knowledge retrieval with LLM generation. RAG systems first retrieve relevant information fragments from external knowledge bases (documents, databases, vector stores), then inject this information as context into LLM prompts, enabling the model to generate answers based on the most current and accurate information.",
      function: "Functions",
      functionContent: "RAG addresses two core LLM challenges:\n‚Ä¢ Knowledge cutoff: LLM training data is time-limited; RAG provides real-time updates\n‚Ä¢ Hallucination: RAG reduces false information by referencing real documents\n‚Ä¢ Domain adaptation: switch knowledge bases without model fine-tuning\n‚Ä¢ Traceability: generated answers can cite specific sources\n‚Ä¢ Cost efficiency: updating knowledge via RAG is cheaper than fine-tuning",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ Enterprise knowledge base Q&A: answer employee questions based on internal documents\n‚Ä¢ Customer support: retrieve product docs and FAQs for accurate support\n‚Ä¢ Legal assistant: retrieve statutes and case law for legal analysis\n‚Ä¢ Code assistant: retrieve codebase and technical docs for development\n‚Ä¢ Real-time queries: retrieve latest news, market data, and time-sensitive information",
      relation: "Relationships",
      relationContent: "RAG is a key knowledge acquisition mechanism for Agents:\n‚Ä¢ Can be exposed as MCP Server retrieval capabilities\n‚Ä¢ Enhances Agent knowledge acquisition, solving LLM cutoff issues\n‚Ä¢ Serves as knowledge retrieval nodes in Workflows\n‚Ä¢ Works with Prompt Engineering ‚Äî retrieved results injected via prompt templates\n‚Ä¢ Provides dynamic knowledge support for Skills beyond static definitions",
    },
    workflow: {
      title: "Workflow",
      subtitle: "Predefined task execution orchestration",
      definition: "Definition",
      definitionContent: "Workflow is a predefined task execution orchestration system that decomposes complex tasks into ordered step sequences, defining execution order, conditional branches, parallel/serial relationships, and data flow between steps. In AI Agent scenarios, Workflows orchestrate the execution of Agents, Sub-Agents, and tools to ensure tasks complete as expected.",
      function: "Functions",
      functionContent: "Workflows provide determinism and controllability in task execution:\n‚Ä¢ Process standardization: codify best practices into reusable templates\n‚Ä¢ Execution control: clearly define what, who, and when for each step\n‚Ä¢ Conditional branching: dynamically adjust subsequent steps based on results\n‚Ä¢ Error handling: define retry, rollback, and compensation mechanisms\n‚Ä¢ Observability: track execution status and results for each step\n‚Ä¢ Parallel orchestration: optimize efficiency with parallel and serial arrangements",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ CI/CD automation: commit ‚Üí build ‚Üí test ‚Üí deploy pipeline\n‚Ä¢ Multi-step data processing: collect ‚Üí clean ‚Üí analyze ‚Üí visualize\n‚Ä¢ Approval processes: submit ‚Üí initial review ‚Üí final review ‚Üí execute\n‚Ä¢ Agent orchestration: Planning Agent ‚Üí Execution Agent ‚Üí Review Agent\n‚Ä¢ Content production: topic selection ‚Üí writing ‚Üí editing ‚Üí review ‚Üí publish",
      relation: "Relationships",
      relationContent: "Workflow is the execution orchestration layer of Agent systems:\n‚Ä¢ Orchestrates Agent and Sub-Agent execution order and dependencies\n‚Ä¢ Defines Agent Team collaboration processes and role interactions\n‚Ä¢ Embeds Prompt templates defining Agent behavior per step\n‚Ä¢ Invokes external tools via MCP Servers as workflow nodes\n‚Ä¢ Collaborates with ACP for cross-Agent task transfer",
    },
    prompt: {
      title: "Prompt Engineering",
      subtitle: "Methodology for designing and optimizing LLM inputs",
      definition: "Definition",
      definitionContent: "Prompt Engineering is a systematic methodology for designing, optimizing, and iterating LLM input prompts. It studies how to craft prompts that guide LLMs to produce high-quality, controllable, expected outputs. Prompt Engineering spans from simple instruction design to complex system prompt architectures and is a core skill in AI application development.",
      function: "Functions",
      functionContent: "Prompt Engineering is the primary means of controlling LLM behavior:\n‚Ä¢ Behavior definition: define Agent roles, capabilities, and constraints via system prompts\n‚Ä¢ Output control: standardize output format, style, and content boundaries\n‚Ä¢ Reasoning guidance: use Chain-of-Thought (CoT) techniques for better reasoning\n‚Ä¢ Few-shot learning: teach LLMs new task patterns through examples\n‚Ä¢ Safety guardrails: design anti-injection and boundary protection strategies",
      scenario: "Use Cases",
      scenarioContent: "‚Ä¢ CLAUDE.md system prompts: define project-level behavior standards for Claude Code\n‚Ä¢ AGENTS.md: configure system instructions and work modes for Codex CLI\n‚Ä¢ Few-shot examples: teach models specific tasks through input-output pairs\n‚Ä¢ Chain of Thought: guide models through step-by-step reasoning\n‚Ä¢ Prompt templates: standardized prompt structures with variable insertion",
      relation: "Relationships",
      relationContent: "Prompt Engineering is the foundational methodology of the AI Agent ecosystem:\n‚Ä¢ Foundation for writing Skills content ‚Äî each Skill is essentially well-crafted prompts\n‚Ä¢ Embedded in Workflows as templates defining Agent behavior per step\n‚Ä¢ All Agent behavior is ultimately driven by prompts\n‚Ä¢ Works with RAG ‚Äî retrieved results injected via prompt templates\n‚Ä¢ Influences how Agents interact via MCP and ACP protocols",
    },
    anthropic: {
      title: "Anthropic Claude Family",
      subtitle: "Safety-focused models with 1M token context (2026)",
      definition: "Model Family Overview (Updated Feb 2026)",
      definitionContent: "Anthropic's Claude series prioritizes AI safety and interpretability, trained with Constitutional AI. Current product lines (as of Feb 2026):\n\n‚Ä¢ Claude Sonnet 4.6 (2026-02-17) ‚Äî Latest Sonnet flagship, major upgrades across coding, agent planning, computer use, and long-context reasoning; 1M token context window (Beta); default on Free & Pro plans\n‚Ä¢ Claude Opus 4.6 (2026-02-05) ‚Äî Strongest reasoning flagship, sustained agentic tasks in large codebases, improved code review & debugging; 1M token context (Beta)\n‚Ä¢ Claude Sonnet 4.5 ‚Äî Previous Sonnet, still widely used, 200K context\n‚Ä¢ Claude Haiku 4 (claude-haiku-4-20251002) ‚Äî Fast and economical for high-concurrency\n‚Ä¢ Claude 3.5 Sonnet / Haiku ‚Äî Generation 3 line, still supported by many proxies",
      function: "Core Capabilities",
      functionContent: "‚Ä¢ Ultra-long context: 1M tokens Beta (Sonnet/Opus 4.6), supports large-scale codebase analysis\n‚Ä¢ Code capabilities: SWE-bench leader, foundation model for Claude Code\n‚Ä¢ Long-running agents: Opus 4.6 designed for sustained multi-step agent tasks in large codebases\n‚Ä¢ Computer Use: Sonnet 4.6 dramatically improved computer operation abilities\n‚Ä¢ Context Compaction: auto-compress long conversation history to maximize context window\n‚Ä¢ Multimodal: image, PDF, document analysis\n‚Ä¢ Tool use: powerful Function Calling / Tool Use, native MCP protocol supporter",
      scenario: "Typical Use Cases",
      scenarioContent: "‚Ä¢ Claude Code ‚Äî official AI coding assistant, claude-sonnet-4-6 / claude-opus-4-6 as core models\n‚Ä¢ Large codebase agents: Opus 4.6 for cross-file refactoring, complex bug fixes\n‚Ä¢ Ultra-long document analysis: legal contracts, papers, 1M-token codebases\n‚Ä¢ Agent building: MCP protocol champion, ideal for tool-intensive Agents\n‚Ä¢ Content creation: technical writing, marketing copy, multilingual translation",
      relation: "Ecosystem Relations",
      relationContent: "‚Ä¢ Initiated the MCP protocol; Claude Code/Gemini CLI/Codex all support MCP\n‚Ä¢ claude-sonnet-4-6 is the recommended target model for Claude Code Providers in this platform\n‚Ä¢ Anthropic API is compatible with OpenAI format; most proxies (PackyCode, Kimi, etc.) support it\n‚Ä¢ Context Compaction requires Claude Code v1.x or claude.ai client support",
    },
    openai: {
      title: "OpenAI GPT / o / Codex Series",
      subtitle: "Most widely adopted general-purpose LLMs (2026 latest)",
      definition: "Model Family Overview (Updated Feb 2026)",
      definitionContent: "OpenAI's product lines continue expanding. As of Feb 2026:\n\n„ÄêGPT-5 Series (General Flagship)„Äë\n‚Ä¢ GPT-5 (Aug 2025) ‚Äî Current most capable general model; built-in Chain-of-Thought reasoning; multimodal (text/image/audio); expert-level intelligence\n‚Ä¢ GPT-5 mini ‚Äî Lightweight GPT-5, significantly cheaper while retaining strong reasoning\n‚Ä¢ GPT-4o ‚Äî Previous multimodal flagship, still widely used\n\n„Äêo Series (Reasoning-Enhanced)„Äë\n‚Ä¢ o3 (Apr 2025) ‚Äî Flagship reasoning model, top in math/code/science; o3-pro for deeper reasoning\n‚Ä¢ o4-mini (Apr 2025) ‚Äî Lightweight reasoning, faster, default model for Codex CLI\n\n„ÄêCodex Series (Code-Specialized)„Äë\n‚Ä¢ GPT-5.3-Codex (Feb 2026) ‚Äî Latest agentic coding flagship, full upgrade over GPT-5.2-Codex in programming and professional agent capabilities\n‚Ä¢ GPT-5.2-Codex ‚Äî Previous Codex generation, still widely used in Codex CLI",
      function: "Core Capabilities",
      functionContent: "‚Ä¢ GPT-5: unified general reasoning + multimodal with built-in thinking, replacing o1's \"slow thinking\" and GPT-4o's \"fast answer\" separately\n‚Ä¢ o3 series: superior Chain-of-Thought, mathematical proofs, competitive coding\n‚Ä¢ GPT-5.3-Codex: designed for agentic coding, computer use + multi-step agent planning\n‚Ä¢ Function Calling / Structured Outputs: most mature tool invocation API and JSON output in the industry\n‚Ä¢ Codex CLI: openai/codex open-source CLI, supports o4-mini or GPT-5.3-Codex backends",
      scenario: "Typical Use Cases",
      scenarioContent: "‚Ä¢ Codex CLI default model (o4-mini), upgradeable to GPT-5.3-Codex\n‚Ä¢ Complex reasoning and math: o3/o3-pro for math competitions, scientific research\n‚Ä¢ General Q&A and writing: GPT-5 / GPT-5 mini for everyday assistance\n‚Ä¢ Code generation & agents: GPT-5.3-Codex is the agentic coding choice\n‚Ä¢ Enterprise: Azure OpenAI for compliant deployment",
      relation: "Ecosystem Relations",
      relationContent: "‚Ä¢ OpenAI API is the de facto industry standard; most third-party proxies are compatible\n‚Ä¢ Codex CLI defaults to o4-mini, also supports GPT-5.3-Codex\n‚Ä¢ This platform supports OpenAI-compatible endpoints as custom Providers",
    },
    google: {
      title: "Google Gemini Family",
      subtitle: "Native multimodal + Gemini 3 Deep Think (2026)",
      definition: "Model Family Overview (Updated Feb 2026)",
      definitionContent: "Google DeepMind's Gemini series was built multimodal from the ground up, with major upgrades in 2026:\n\n‚Ä¢ Gemini 3 Deep Think (Feb 2026) ‚Äî Major upgrade for frontier science, research, and engineering challenges; blends deep scientific knowledge with everyday engineering utility\n‚Ä¢ Gemini 2.5 Pro ‚Äî Advanced reasoning flagship, 1M token context (2M experimental), officially available on Vertex AI\n‚Ä¢ Gemini 2.5 Flash ‚Äî Best speed/cost ratio, still the top choice for high concurrency\n‚Ä¢ Gemini 2.0 Flash ‚Äî Previous speed champion, stable production-ready, widely used by middleware\n‚Ä¢ Gemini Nano ‚Äî On-device lightweight model (Android / Chrome)",
      function: "Core Capabilities",
      functionContent: "‚Ä¢ Ultra-long context: 1M tokens (Gemini 2.5 Pro), analyze entire codebases or long videos\n‚Ä¢ Gemini 3 Deep Think: deep scientific reasoning, breakthrough across science and engineering\n‚Ä¢ Native multimodal: unified text, image, audio, video, PDF processing\n‚Ä¢ Google Search grounding: built-in real-time search integration\n‚Ä¢ Code capability: Gemini 2.5 Pro approaches Claude Sonnet on LiveBench code benchmarks\n‚Ä¢ Google Workspace integration: deep integration with Docs/Sheets/Gmail",
      scenario: "Typical Use Cases",
      scenarioContent: "‚Ä¢ Gemini CLI's underlying model (free tier: 60 req/min, 1,000 req/day)\n‚Ä¢ Frontier science & research: Gemini 3 Deep Think for complex engineering and scientific analysis\n‚Ä¢ Ultra-long document analysis: video understanding, multi-document comparison\n‚Ä¢ Google ecosystem integration: Workspace AI assistant\n‚Ä¢ Multimodal tasks: chart analysis, code screenshot interpretation",
      relation: "Ecosystem Relations",
      relationContent: "‚Ä¢ Official default model family for Gemini CLI (Gemini 3 series)\n‚Ä¢ Gemini CLI Provider maps to Google AI Studio API or Vertex AI\n‚Ä¢ Configured via GOOGLE_API_KEY / GEMINI_API_KEY environment variables",
    },
    meta: {
      title: "Meta Llama Family",
      subtitle: "Most influential open-source LLMs (2026 latest)",
      definition: "Model Family Overview (Updated 2026)",
      definitionContent: "Meta's Llama series is the most important open-source LLM family, continuously evolving through 2025-2026:\n\n‚Ä¢ Llama 4 (2026) ‚Äî Next-generation flagship with significantly enhanced multimodal capabilities, stronger reasoning and Agent abilities\n‚Ä¢ Llama 3.3 70B ‚Äî Most widely used open-source version, approaching GPT-4o quality\n‚Ä¢ Llama 3.1 405B ‚Äî Top-tier open-source model, massive parameter count\n‚Ä¢ Llama 3.2 (Multimodal) ‚Äî Image understanding support, includes 1B/3B lightweight variants\n‚Ä¢ Code Llama ‚Äî Derivative optimized for code generation",
      function: "Core Capabilities",
      functionContent: "‚Ä¢ Fully open source: weights freely downloadable for commercial use, local deployment\n‚Ä¢ Fine-tuning friendly: LoRA / QLoRA fine-tuning at very low cost\n‚Ä¢ Tool calling: Function Calling supported from Llama 3.1+\n‚Ä¢ Size range: 1B to 405B+, fits all hardware tiers\n‚Ä¢ Active ecosystem: Ollama, LM Studio, vLLM and more\n‚Ä¢ Llama 4: enhanced multimodal + improved Agent support",
      scenario: "Typical Use Cases",
      scenarioContent: "‚Ä¢ Local private deployment: Ollama + Llama 3.3 70B, fully offline\n‚Ä¢ Enterprise private deployment: data stays on-premises, compliance-friendly\n‚Ä¢ Fine-tuning: train on domain-specific data for specialized models\n‚Ä¢ Research: study LLM internals and experiment freely",
      relation: "Ecosystem Relations",
      relationContent: "‚Ä¢ Can be run locally with Ollama and connected via OpenAI-compatible interface\n‚Ä¢ Groq and similar inference services provide Llama API access\n‚Ä¢ Many third-party providers (Together AI, Perplexity) offer Llama APIs",
    },
    chinese: {
      title: "Chinese Leading LLMs",
      subtitle: "Top domestic models comparison (Feb 2026 latest)",
      definition: "Major Vendors Overview (Updated Feb 2026)",
      definitionContent: "China's LLM ecosystem continues rapid growth, with multiple new models released around Spring Festival 2026:\n\n„ÄêAlibaba Qwen Series„Äë\n‚Ä¢ Qwen3 ‚Äî Latest flagship, fully surpasses Qwen2.5 in reasoning and code\n‚Ä¢ Qwen2.5 / Qwen2.5-Coder ‚Äî Still widely used previous generation\n‚Ä¢ QwQ ‚Äî Reasoning-enhanced, optimized for math and logic\n\n„ÄêDeepSeek Series„Äë\n‚Ä¢ DeepSeek R2 ‚Äî Latest reasoning flagship, surpasses R1, approaches top closed-source models\n‚Ä¢ DeepSeek V3.2 ‚Äî General flagship upgrade, extremely cost-effective\n‚Ä¢ DeepSeek-R1 ‚Äî Previous reasoning model, fully open source\n\n„ÄêKimi (Moonshot)„Äë\n‚Ä¢ kimi-for-coding ‚Äî Claude Code / Codex compatible proxy, built-in template in this platform\n‚Ä¢ Moonshot v1 ‚Äî General-purpose model\n\n„ÄêOthers„Äë\n‚Ä¢ Baidu ERNIE 4.5, Zhipu GLM-4, MiniMax-Text-01, Hunyuan, etc.",
      function: "Core Advantages",
      functionContent: "‚Ä¢ Price: DeepSeek V3.2 is among the cheapest APIs globally; domestic models far undercut OpenAI pricing\n‚Ä¢ Chinese language: outstanding Chinese comprehension, generation and cultural awareness\n‚Ä¢ Open source: DeepSeek R2, Qwen3 fully open source with local deployment support\n‚Ä¢ Compliance: data stored domestically, meeting data sovereignty requirements\n‚Ä¢ Reasoning breakthrough: DeepSeek R2 / QwQ now approach o3-level reasoning ability",
      scenario: "Typical Use Cases",
      scenarioContent: "‚Ä¢ kimi-for-coding: directly replaces Claude API for Claude Code local development\n‚Ä¢ DeepSeek V3.2: ultra cost-effective general tasks with Function Calling support\n‚Ä¢ Qwen3 / Qwen2.5-Coder: code generation and review with Chinese comment support\n‚Ä¢ Enterprise private deployment: Qwen3 / DeepSeek local deployment, data stays on-premises",
      relation: "Platform Integration",
      relationContent: "‚Ä¢ This platform has built-in PackyCode Provider template supporting kimi-for-coding compatible endpoints\n‚Ä¢ Any OpenAI-compatible domestic model API can be added as a Custom Provider\n‚Ä¢ The exported settings.json env.ANTHROPIC_BASE_URL can point to domestic proxy addresses",
    },
    specialized: {
      title: "Specialized & Frontier Models",
      subtitle: "Vertical-domain and emerging-architecture leaders (2026)",
      definition: "Category Overview (Updated Feb 2026)",
      definitionContent: "Beyond general LLMs, specialized and frontier models define the cutting edge in 2026:\n\n„ÄêAgentic Coding Specialists„Äë\n‚Ä¢ GPT-5.3-Codex (OpenAI, Feb 2026) ‚Äî Latest agentic coding flagship, optimized for multi-step Agent tasks\n‚Ä¢ Claude Opus 4.6 (Anthropic, Feb 2026) ‚Äî Best for long Agent tasks in large codebases\n‚Ä¢ GitHub Copilot (GPT-5 powered) ‚Äî Most widely used code completion tool\n\n„ÄêMultimodal Frontier„Äë\n‚Ä¢ GPT-5 (real-time mode) ‚Äî Built-in thinking + real-time multimodal interaction\n‚Ä¢ Gemini 3 Deep Think ‚Äî Deep reasoning for science, research, and engineering\n‚Ä¢ Claude Sonnet 4.6 ‚Äî Dramatically improved Computer Use capabilities\n\n„ÄêReasoning-Enhanced„Äë\n‚Ä¢ OpenAI o3 / o3-pro ‚Äî Deep Chain-of-Thought, top in math and science\n‚Ä¢ DeepSeek R2 ‚Äî Open-source reasoning flagship, approaching o3\n‚Ä¢ QwQ (Qwen Reasoning) ‚Äî Best Chinese reasoning model\n\n„ÄêSmall but Powerful„Äë\n‚Ä¢ Phi-4 (Microsoft) ‚Äî 14B params, outperforms much larger models in reasoning\n‚Ä¢ Gemma 3 (Google, 2026) ‚Äî Next-gen open-source small models\n‚Ä¢ Mistral Small 3 ‚Äî Efficient multilingual lightweight model",
      function: "Model Selection Guide (2026)",
      functionContent: "How to choose the right model for your use case:\n\n‚Ä¢ Strongest reasoning/math ‚Üí OpenAI o3-pro / DeepSeek R2\n‚Ä¢ Best agentic coding ‚Üí Claude Opus 4.6 / GPT-5.3-Codex\n‚Ä¢ Ultra-long documents ‚Üí Gemini 2.5 Pro (1M tokens) / Claude Sonnet 4.6 (1M Beta)\n‚Ä¢ Local private deployment ‚Üí Llama 4 / Qwen3 / DeepSeek V3.2\n‚Ä¢ Lowest API cost ‚Üí DeepSeek V3.2 / Gemini 2.5 Flash / Qwen3\n‚Ä¢ Best Chinese ‚Üí Qwen3 / DeepSeek V3.2 / Kimi\n‚Ä¢ CLI coding assistant ‚Üí Claude Code (Sonnet 4.6) / Codex (o4-mini / GPT-5.3-Codex)",
      scenario: "Frontier Trends (2026)",
      scenarioContent: "‚Ä¢ 1M token context now standard: Claude Sonnet/Opus 4.6 and Gemini 2.5 Pro all support 1M+ tokens\n‚Ä¢ Agentic coding specialization: GPT-5.3-Codex / Claude Opus 4.6 purpose-built for long Agent tasks\n‚Ä¢ Reasoning models mainstream: o3/DeepSeek R2/QwQ now part of everyday development workflows\n‚Ä¢ Open source matches closed source: Qwen3 / DeepSeek R2 reach closed-source top-tier quality\n‚Ä¢ Scientific AI breakthrough: Gemini 3 Deep Think targets frontier science and engineering challenges",
      relation: "Ecosystem Compatibility",
      relationContent: "‚Ä¢ Most models provide OpenAI-compatible APIs, directly configurable as Custom Providers\n‚Ä¢ Recommended to use Ollama (local) or OpenRouter (cloud aggregator) for multi-model access\n‚Ä¢ Claude Code supports any compatible proxy via ANTHROPIC_BASE_URL\n‚Ä¢ Codex CLI supports custom endpoints via config.toml provider_base_url",
    },
  },
  theme: {
    light: "Light",
    dark: "Dark",
    system: "System",
  },
};
